Almost all interpretability methods work on image classification tasks. To learn how these methods are applied, how they work and what output they generate, the first step in this work is applying these methods on a classification task. We chose a dataset from the medical imaging field: The NIH (National Institutes of Health, United States) chest X-ray dataset \cite{wang2017chestx}.

We downloaded the dataset, trained a neural network on the dataset and applied the selected methods described above.

\section{NIH Chest X-ray dataset}
The NIH Chest X-ray dataset contains 112,120 X-ray scans from 30,805 unique patients \cite{nihchestxraykaggle}. Every scan has one or more disease labels. Figure \ref{chest_xray_sample} show three sample images from the dataset.

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{chapters/03_classification/images/chest-x-ray.png}
\caption{Examples for the NIH Chest X-ray dataset}
\label{chest_xray_sample}
\end{figure}

\section{Model Training}
We decided to train a model based on an existing neural network architecture, because building a state of the art architecture is very hard and not the focus of this thesis.

The detailed results of the training sessions are available in the results directory of the GitHub repository: \href{https://github.com/andef4/thesis-code/tree/master/nhs-chest-xray/results/}{nhs-chest-xray/results}.

\subsection{Inception ResNet v2}
\nblink{nhs-chest-xray/inception\_resnetv2.ipynb}

We chose Inception ResNet v2 \cite{szegedy2017inception} as the architecture. This is modern neural network built for image analysis and generally trained on the ImageNet dataset.
The ResNet variant of the Inception architecture delivers similar performance as the normal Inception model, but with significantly reduced training time.

\nblink{nhs-chest-xray/preprocess.ipynb}
The preprocessing steps required to use this architecture are resizing the images to 299x299 pixels and converting the gray-scale images to color images.
We also tried to modify the network to directly use gray-scale images, but this was not successful because the network architecture is built for three channel images.

We initially trained the network on a smaller subset (5607 image) of the dataset. The maximum reached validation accuracy was 40\%. The training times for the small training subset
were already very long, we therefore decided to abandon the Inception architecture for now and use a ResNet based architecture instead.

\subsection{ResNet}
\nblink{nhs-chest-xray/resnet.ipynb}

The first test of ResNet50 \cite{he2016deep} with pretrained parameters (trained on ImageNet) on the sample dataset showed fast training times but low accuracy. Training the network from scratch showed promising validation accuracy, which started to decrease on later epochs. The training accuracy was still increasing, this is therefore a clear indicator that the neural network is overfitting. We decided to change the architecture to ResNet18 which is a smaller ResNet variant with fewer parameters and should be less susceptible to overfitting.

The first training run of ResNet18 with the sample dataset looked promising, so we moved to train the network on the full dataset. The results were underwhelming, with the validation accuracy maxing out at 23\%.

\subsection{Single label only}
The NIH Chest X-ray dataset is a multi class dataset. This means a single image can contain labels for multiple disease. Training models for such datasets is much harder than training datasets where each image only has one label. We therefore decided to remove all images which contain multiple labels and only train on images with one label.

We did multiple iterations on the full dataset with ResNet18, changing multiple parameters:
\begin{itemize}
    \item Running on sample dataset vs. full dataset
    \item Using SGD optimizer instead of Adam
    \item Using pretrained model vs. from scratch model
    \item Use data augmentation on the input (color jitter, random horizontal flip)
\end{itemize}

None of these parameters changed the validation accuracy significantly, always peaking around 60\% to 65\% training accuracy.

\subsection{Only images with actual diagnosis}
The dataset also contains images which have the class "No findings". This is an important class for training a neural network that should correctly diagnose chest X-ray scans. But for our project, images with actual findings are much more interesting. We decided to filter out these images.

With this change, the validation accuracy dropped to around 30\%, even when running on the full dataset.

\subsection{Densenet}
\nblink{nhs-chest-xray/densenet.ipynb}

Researching what other people did to train a model for this dataset, we chame across the Che
Densenet


% https://medium.com/@jrzech/reproducing-chexnet-with-pytorch-695ff9c3bf66
Loading the saved model did not work, because the used version of PyTorch was old and incompatible with ours.
=> Retraining

\subsection{Densenet single}
\nblink{nhs-chest-xray/densenet\_singleclass.ipynb}
\nblink{nhs-chest-xray/inception\_resnetv2\_singleclass.ipynb}

\begin{itemize}
    \item Fail: Not correctly setting up classes (last layer has 1000 classes for imagenet, we have 14)
    \item Fail 2: Train and validation set do not have same classes because of ordering in dataset loading code
\end{itemize}

After training for a short time with the fixes, we get an acceptable accuracy rate of 55\%. We then started to calculate the accuracy per class, as has been done on the reproduce-chexnet discussed above. We quickly discovered that only the "No findings" class was accurate, an all other classes had zero correct classifications.
