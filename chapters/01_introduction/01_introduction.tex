\section{Why interpretability}
Modern machine learning models based on deep neural networks are achieving remarkable performance in many fields. In comparison with classic machine learning technologies like decision trees, it is much harder to explain how these neural networks came to their conclusions, because they use thousands to millions of trained parameters.

Especially in the medical imaging field, it is very important that algorithms not only generate a correct diagnosis when training the algorithm, but also show that they are using the same cues in images as trained physicians. These cues are found and verified in scientific studies, and are therefore well understood and proven to be correct. A wrong diagnosis generated by a neural network reduce the confidence of physicians using the technology and can be life threatening when used without professional supervision.

\section{Image classification}
In recent years, many methods for the interpretability of deep (convolutional) neural networks have been proposed, e.g. LIME \cite{ribeiro2016should}, RISE \cite{Petsiuk2018rise}, Grad-CAM \cite{selvaraju2017grad} or DeepLIFT \cite{shrikumar2017learning}. Some example outputs of these methods are shown in Figure \ref{classification_methods}.

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{images/tusker_saliency.png}
\caption{Examples of some interpretability methods for image classification. All examples try to explain how the network identified the class "Tusker" on the image.. \cite{visualattribution}}
\label{classification_methods}
\end{figure}

These methods focus on providing explanations and interpretability of classification problems for datasets like ImageNet \cite{imagenet_cvpr09} or MNIST \cite{lecun1998gradient}. Classification means that an algorithm can tell what is displayed on an image, e.g. if and what kind of disease is visible on a X-ray or MRI scan.

\section{Image segmentation}
Image segmentation is different from classification in the way that the algorithms do not detect what is visible on a picture, but instead mark a region (the segment) in an image where they think something is visible. For example in self driving cars, it is important to know where other cars or pedestrians are located. 

In the medical imaging field, one application is the segmentation of tumors in MRI scans. Figure \ref{introduction_tumor} shows a brain MRI scan with the tumor segment overlaid.

\begin{figure}[h]
\centering
\includegraphics[width=6cm]{chapters/01_introduction/images/tumor_segment.png}
\caption{T1 modality of an MRI brain scan with tumor segment (red) overlaid.}
\label{introduction_tumor}
\end{figure}


\section{Interpretability of image segmentation models}
The interpretability of image segmentation task is nearly in-existent. In many cases this is understandable, because the interpretability methods would generate an image very similar to the actual segment. For example, when detecting pedestrians for self driving cars, the interpretability method would just mark the pedestrian. Only in cases where the network has not yet reached a good accuracy, other pixels in the image would be marked by the method.

In other fields like medical imaging, it is possible or even desirable that a neural network looks at other parts of an image to decide if a part should be segmented. An example for this is the search for tumors on MRI scans of the human brain. The human brain is physically symmetric. When a tumor is growing on one side of the brain, the brain is no longer symmetric. A physician and possibly also neural networks can use this property to detect and segment tumors.

\section{Goals}
The goal of this thesis is to take existing methods for image classification interpretability and modify them so they can work on image segmentation tasks. This includes the following tasks:
\begin{itemize}
    \item Research existing methods for image classification
    \item Asses if the found methods can be modified for image segmentation
    \item Modify the methods for image segmentation
    \item Build and train a neural network on the BraTS brain tumor segmentation dataset
    \item Apply the modified methods on the trained neural network
    \item Analyze, evaluate and discuss the results
    \item Build a reusable Python library so other developers can easily use our modified messages
    \item Provide images and visualizations for teaching material
\end{itemize}

See \autoref{appendix_a} for the full requirements specification.

\section{Constituent}
The constituent of this thesis is Mauricio Reyes and his team from the Medical Faculty of the University of Bern. Mauricio Reyes is head of the Healthcare Imaging A.I. group. This thesis should help him and his team of Master and PhD students to better understand the models they are developing.

\section{Source Code}
The source code of the thesis is available on GitHub:
\begin{itemize}
    \item Thesis source code (mostly Jupyter notebooks): \url{https://github.com/andef4/thesis-code}
    \item Python library source code: \url{https://github.com/andef4/interpret-segmentation}
    \item Latex source of this document: \url{https://github.com/andef4/thesis-doc}
\end{itemize}

All source code is licensed under the permissive MIT license.

\section{Link to Jupyter notebooks}
Many sections contain direct links to the Jupyter notebooks which contain the code to generate the results of the section. They look like this:

\nblink{brats/22a\_testnet\_hdm\_circles\_fixed.ipynb}

The links link directly to the specific notebook in the git repository on GitHub. The notebook is rendered by GitHub and all code and inline images should be visible in a modern web browser.