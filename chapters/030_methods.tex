\section{Whitebox and blackbox methods}
One of the main questions we have to answer is which methods for interpretability can be used on an Image Segmentation task.
Generally there are two types of methods for interpretability: Blackbox and whitebox.
Blackbox methods like LIME or RISE do not need an understanding of the underlying model and can therefore be used on arbitrary network architectures and even non-deep learning technologies like decision trees.
Whitebox methods need to have access to the underling model, because they analyze a certain part of the network (e.g. extracted features).

\subsection{Blackbox}
Blackbox methods modify the input image, run it trough the network and then analyze how the classification output of the network has changed with the modification.
We think it is feasible to apply this method to image segmentation too, but instead of looking how the classification output changes we check how the segmentation
output changes.

\subsection{Whitebox}
Whitebox methods require access to the architecture of the model and therefore only work on some specific architectures classes.

\section{Methods overview}


https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066


\begin{tabular}{| p{7cm} | p{2.5cm} | p{6cm} | }
\hline
\textbf{Method} & \textbf{Model blackbox} & \textbf{Works with PyTorch} \\ \hline
RISE\cite{Petsiuk2018rise} & Yes & Supported out of the box \\ \hline
LIME\cite{ribeiro2016should} & Yes & Library independent \\ \hline
Layer-wise Relevance Propagation (LRP) & No & Should be possible  https://github.com/Hey1Li/Salient-Relevance-Propagation missing batch norm \\ \hline
DeepLIFT & No & Initial implementation in SHAP\cite{NIPS2017_7062} \\ \hline
Prediction Difference Analysis \cite{todo} & ? & ? \\ \hline
PatternNet & ? & ? \\ \hline
Deep Taylor & ? & ? \\ \hline
Grad-CAM (Selvaraju 2017) & No & Many implementations \\ \hline
SHAP DeepExplainer\cite{NIPS2017_7062} & ? & ? \\ \hline
SHAP KernelExplainer\cite{NIPS2017_7062} & ? & ? \\ \hline
Meaningful Perturbation (Fong 2017)\cite{todo} & Yes & Implementation exitst \cite{todo} https://github.com/jacobgil/pytorch-explain-black-box \\ \hline
Excitation Backprop (Zhang 2016)\cite{todo}  & ? & ? \\ \hline
\end{tabular}

% many implementations: https://github.com/yulongwang12/visual-attribution

fong has many references

% https://github.com/marcoancona/DeepExplain/blob/master/docs/comparison.png
good images: fong 2017

Baehrens'10 Gradient
Sundarajan'17 Int Grad
Zhou'16 GAP
Haufe'15 Pattern
Symonian'13 Gradient
Zeiler'14 Occlusions
Zurada'94 Gradient
Poulin'06 Additive
Zeiler'14 Deconv
Landecker'13 Contrib Prop
Caruana'15 Fitted Additive
Bazen'13 Taylor
Springenberg'14 Guided BP


%Zintgraf'17 Pred Diff  https://arxiv.org/pdf/1702.04595.pdf
%Ribeiro'16 LIME
% Fong'17 M Perturb
% Lundberg'17 Shapley
%Montavon'17 Deep Taylor
%Bach'15 LRP
%Kindernnans'17 PatternNet
%Shrikumar'17 DeepLIFT
%Zhang'16 Excitation Backprop
%Selvaraju'17 Grad-CAM

\section{Method selection}
\section{RISE}
Similar technology as LIME, uses PyTorch. Newer than LIME, paper asserts to be better.

Written for PyTorch

\section{LIME}
\section{Grad-CAM}









