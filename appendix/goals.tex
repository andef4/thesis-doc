\chapter{Appendix A: Requirements specification}
\section{Introduction}
Modern machine learning models based on deep neural networks are achieving remarkable performance in many fields, including medical image analysis. In comparison with classic machine learning technologies like decision trees, it is much harder to explain how these neural networks came to their conclusions, because they use thousands to millions of training parameters.

In recent years, many methods for the interpretability of deep (convolutional) neural networks have been proposed, e.g. LIME \cite{ribeiro2016should}, RISE \cite{Petsiuk2018rise}, Grad-CAM \cite{selvaraju2017grad} or DeepLIFT \cite{shrikumar2017learning}.

\begin{figure}[h]
\centering
\caption{Examples of some interpretability methods for image classification \cite{visualattribution}}
\includegraphics[width=14cm]{images/tusker_saliency.png}
\end{figure}

These methods focus on providing explanations and interpretability of classification problems for datasets like ImageNet \cite{imagenet_cvpr09} or MNIST \cite{lecun-mnisthandwrittendigit-2010}. Classification means that an algorithm can tell what is displayed on an image. ImageNet for example contains over 21,000 categories and 14 million images.

What has not yet been researched much is the interpretability of image segmentation tasks. With image segmentation, an algorithm does not only detect objects in images, but also marks (segments) where it found the specific object(s) in the image. On many image segmentation tasks, interpretability based on image regions like Grad-CAM (see image above) do not make that much sense, because the segmentation itself already returns a region of an image.

\begin{figure}[H]
\centering
\caption{Image segmentation on MRI scans of the brain \cite{soltaninejad2017automated}}
\includegraphics[width=10cm]{images/brain_segmentation.jpg}
\end{figure}

On other problems like finding and segmenting brain tumors on MRI images, medical professionals do not only look at the exact region where a tumor resides, but also at other regions of the brain. On the upper row of the scan above it is clearly visible that the tumor on the right side is extending the space used for the right half of the brain, and therefore many brain features are not symmetrical any longer. A neural network trained on this task should also use these cues to correctly segment the tumors. 

The goal of this thesis is to provide such a tool by modifying existing interpretability methods for classification tasks to work on image segmentation tasks. The tool generates images with marked regions similar to the ones in Figure 1.1. The marked regions represent the regions where the neural network took cues to generate the image segments as seen in Figure 1.2.

The customer for this thesis is Mauricio Reyes from the Medical Faculty of the University of Bern. He and his team work in the medical imaging field together with the Inselspital Bern. One of the bigger projects is building a method to automatically segment brain tumors. The work of this thesis will help the team to understand what their machine learning model is doing and increase the trust of medical professionals into the methods because they can understand what the machine learning model is doing.

\section{Project planning overview}

The high-level approach for this thesis is:
\begin{itemize}
    \item Talk with the customer (Mauricio Reyes) to find out what his expectations are
    \item Use the current machine learning methods for interpretability on a simple classification task to learn what methods are available, how they work and and how they can be applied
    \item Train a neural network for brain tumor segmentation. This provides a real-world test bed for our further work
    \item Modify existing interpretability methods so they work on image segmentation tasks
    \item Provide a reusable Python library so other users can easily apply our developed methods
\end{itemize}

The author of this thesis is a part-time student normally working 60\% for a company. For this thesis he took a 6 week unpaid leave to work full-time on it.
Therefore a big part of the thesis should be completed in this 6 weeks. The 6 weeks start in the second week of the thesis.

In this 6 weeks, the main focus is writing the code to accomplish the defined goals in this document. The rest of the time will be used to finish the thesis document, package up the built program so it can easily be used by other people and complete all the other tasks required for the thesis (poster, movie, presentation, defense preparation etc.).

In the first 6 weeks the author plans to meet with the tutor (Peter von Niederhäusern) and the customer (Mauricio Reyes) each week to report on project progress and to get input on the thesis.
After the first six weeks, biweekly meetings are planned.

At least two meetings with the expert Andreas Spichiger will be scheduled.

For detailed time planning information, please consult the Gantt chart at the end of this document.

\section{Goals and requirements}

\subsection{Overview}
The resulting interpretability toolkit for image segmentation tasks takes the form of a Python library. The library should provide an easy to use interface for researchers to apply
the modified methods on their own neural network models and datasets.

\subsection{Functional requirements}
\subsubsection{Required goals}

\textbf{Build classification model and apply methods}
As a first step, a neural network should be trained on an image classification task in the medical imaging field.
After that, the existing interpretability methods RISE, LIME and Grad-CAM are applied on the trained model.
The goal of this step is to learn how these methods work, how they are applied and how their output looks like.

\textbf{Image segmentation model training}
To have a real world test bed to apply the newly developed methods, a neural network for image segmentation is trained.
The dataset used for this network is BraTS 2018, a dataset containing MRI scans of human brains which contain tumors.
The trained neural network should have an acceptable accuracy to generate meaningful output from the applied methods, but
does not have to be on the same level as current state of the art models. 

\textbf{Modify methods}
The main goal of the thesis is to modify the methods RISE and Grad-CAM to work on image segmentation tasks.
The two methods were chosen because they provide very good results, are widely used and there exist multiple high
quality implementations.

RISE is a black box method, which means it does not need to have access to the neural network itself,
just to the input images and the output of the network.

Grad-CAM is a white box method. It needs access to some of the layers of the neural network. The implication of this
is that applying the method requires configuration for a specific network architecture.

How these two methods can be modified to work on image segmentation instead of classification is currently unclear and therefore the main research topic of this thesis.

\textbf{Library and documentation}
To make the modified methods from above available to other researchers, a Python library will be built. Ideally, a library user can call a single function
which generates the desired output. Documentation of the provided functionality has to be written and examples how to use the library should be made available. The library should also be published on PyPI and Anaconda forge for easy installation by the standard package managers in the
Python and machine learning community.

\subsubsection{Optional goals}
The methods modified above (RISE and Grad-CAM) are black box and white box methods, respectively.
If a way is found how to modify these two methods to work with image segmentation tasks, it should be possible to modify other
methods in the same class (black/white box) in a similar way. These other methods will be chosen based on quality of output and availability and quality of existing implementations.

These methods will also be added to the Python library.

\subsection{System requirements}
The system requirements for the resulting Python library are:

\begin{itemize}
    \item Works with the PyTorch machine learning library
    \item Supports commonly used neural network architectures ResNet and densenet out of the box
    \item Supports other neural network architectures with minor modifications
    \item Does not depend on the underlying platform, only on a working Python installation with PyTorch
    \item Can optionally use Nvidia GPUs to speed up computation, but must also work on CPUs (with potentially greatly reduced performance)
    \item Can be installed with standard Python package managers (pip and Anaconda)
\end{itemize}

\subsection{Domain requirements}
The work in this thesis is done in the medical imaging field, because the customer comes from this field and because the problems in the field can benefit from the
work done in this thesis. The resulting Python library from this thesis will not be specific to the medical imaging field and should be applicable on all image segmentation tasks which take cues from parts of the image which are not contained in the resulting segmentation region.

\section{Detailed project plan}
\subsection{NIH Chest X-ray}
To get a general overview how the different interpretability methods work, we plan to build a model for a medical imaging task and apply some of the more commonly used methods on it. We will also search for existing pretrained models to complete this work faster and/or to have a more accurate model. The model will reuse an existing convolutional neural network architecture, e.g. ResNet or Inception.

The dataset selected for this task is the NIH Chest X-ray dataset \cite{wang2017chestx}. It contains 120,000 grayscale x-ray scans of chests and labels ranging from no findings to one or more diagnoses.

After building the model, we evaluate some of the common methods on the trained models. We want to determine which methods are independent of the used network architecture (methods that view the model as a black box). We also want to find implementations of the methods written for PyTorch or alternatively estimate how easy the methods could be ported to PyTorch. The last step is to show how the output of the method looks like and (optionally) check with a medical professional which output they think would be most helpful for the interpretability of a method.

\subsection{Brain tumors}
To build the methods to interpret image segmentation tasks, we need a real world task. Brain tumor segmentation is an interesting problem in the medical image field. Segmenting brain tumors manually is a tedious process, because MRI machines generate many slices of the brain which then have to be looked at one by one. Automating the process with machine learning will give medical professionals more time to do other tasks and also gives them more confidence in their manual work when supported by an automated model.

To build a working machine learning model, some medical background is required. We will investigate what types of brain tumors exist, how they look like on a scan and what scanner types are available (e.g. MRI/PET/CT). We also investigate how a tumor looks like on a specific scanner type.

\subsection{The BraTS 2018 dataset}
The BraTS \cite{menze2015multimodal} dataset contains MRI scans of the human brain and segmentation labels for found tumors in the scans.
The scan is 3D, saved as horizontal 2D slices. Every slice has four different black/white channels, each with a different setting of the MRI scanner.
There are 4 different labels for every slice, representing 4 different sizes of a tumor.

The tasks for this step is to figure out the data format of the dataset and then load the dataset into memory. Displaying some slices will show if the data was loaded correctly. Next, we extract multiple slices per scanned brain. We only use slices which contain visible tumors. This way we have 2D images we can feed into the convolutional neural network used for image segmentation.

The next step is loading the labels of the data. As recommended by Alain Jungo of the University of Bern, we will merge the two innermost region labels into one label and discard the other two labels.

\subsection{Model for the BraTS dataset}
A widely used architecture for segmentation (especially in the medical imaging field) is the U-Net \cite{ronneberger2015u}.

\begin{figure}[H]
\centering
\caption{The U-Net architecture}
\includegraphics[width=10cm]{images/unet.png}
\end{figure}

The architecture uses a so called "encoder" network on the left side which generates features. A standard convolutional neural network can be used as an encoder. On the right side, instead of a linear layer mapping features generated from the convolutions to classes, a "decoder" network is used. The decoder network generates the output segmentation pixels based on the features extracted by the encoder.

We will build a U-Net using a standard convolutional neural network architecture. The fast.ai library contains a dynamic U-Net Python class which can automatically generate the decoder part of a U-Net by analyzing the CNN. We will evaluate if this works good enough or if a custom architecture is required.

\subsection{RISE method}
Randomized Input Sampling for Explanations (RISE) is a interpretability method that uses the model as a black box. It does not need to know anything about the underlying machine learning model, it only needs access to the input image and the probabilities of the output classes.

RISE generates a series of masks which get applied to the input image. The modified image is then run through the model and the changed probabilities of the output classes are recorded. From the probabilities of the different masks, a heatmap is generated which shows which parts of the image influence the correct classification of the image the most.

To use RISE for image segmentation, we have to find a way to replace the class probabilities with the segmentation pixels. This is part of the main research work of this thesis.


\subsection{Grad-CAM method}
The Grad-CAM \cite{selvaraju2017grad} (Gradient-weighted Class Activation Mapping) is a white box method which uses the parameters of a learned model to output a heatmap for a specific image classified by the network.

Grad-CAM is a widely used method with many high quality implementations which also generated an easy to understand output which can be used by medical professionals.

As with RISE, Grad-CAM is built for image classification tasks. Because the encoder part of a U-Net is a standard convolutional neural network, it should be possible to apply Grad-CAM on this part of the U-Net. If and how this is possible is part of the main research topic of this thesis.


\subsection{Library}
If the modification of the methods do work with image segmentation tasks, it is planned to build a Python library which allows to easily apply the methods to other datasets by its users. The library should be usable on PyTorch models. Documenting the library (e.g. on readthedocs.io) is essential for potential users, as are examples how to use the library, which can be extracted from the thesis source code.

Also planned is to publish the library on PyPI and conda-forge, so researchers can install the library in their environments.

\subsection{Other black box model methods (optional)}
There are other black box methods (e.g. LIME \cite{ribeiro2016should}) which work similar to RISE. If a successful method is found to apply RISE to image segmentation tasks, modifying these other methods to also work for segmentation should not be a major problem.

Building a high quality solution with RISE has a higher priority than adding additional methods.

\subsection{Other white box model methods (optional)}
As described above, there are many white box model methods which can be applied to this problem. If we can find a solution how to apply Grad-CAM, using other methods like PatternNet \cite{kindermans2017learning} or Meaningful Perturbation \cite{fong2017interpretable} should be possible too.

\section{Evaluation and testing}
\subsection{Model evaluation}
There are many metrics to decide if a neural network model returns good results or not.
The metrics used in this thesis will be loss, Dice coefficient/F1 score and the Hausdorff distance.

The loss describes how sure a network is with a given answer. An answer may be accurate, but the network is very unsure about it. The loss is especially useful to see if a neural network is learning something when training it. The loss in a training session should decrease with time, even when the accuracy is still very bad.
Another insight the loss can give is if the neural network overfits, i.e memorizes the input data instead of finding patterns in the data. If the training loss is going down (the loss calculated on the data which is used for training) but the validation loss (loss calculated on data the network has never seen before, which is only used to test the network performance) is going up, the network is starting to overfit.

The Dice coefficient (also known as the F1 score) is a metric commonly used on image segmentation tasks. We can interpret every pixel in the segmented image as a boolean value and compare it to the expected segmentation image from the training data. This gives us a standard confusion matrix with true positive, false positive, true negative and false negative, from which we can calculate the coefficient. The coefficient can also be used as a loss function by using the floating point output values of the network as input instead of converting the network output to a black/white pixel image.

The Hausdorff distance is a more advanced metric. It does not only consider if a pixel from the labeled data is in the network output or not, but also takes the surroundings of the expected pixel into account. This means if an expected pixel is not in the network output, but just one pixel to the right of the expected one, this is considered much better than when the pixel is far away or does not exist at all.

\subsection{Testing}
Automatic testing for this thesis is very challenging, because it is difficult to decide if a specific interpretability method returns correct data, even for a human.

It is therefore very important to do thorough manual testing by looking at many examples generated by the methods. Cross checking the generated results with medical professionals is also important, because the output could make sense from a computer scientist's view, but is very wrong from a medical standpoint.

For the resulting Python library, unit and integration tests will be written with the main purpose of avoiding regressions and breaking client code. Integration tests are also useful as examples for the end user.

\subsection{Code quality}
The code quality and the observing of coding conventions and code style guides will be enforced by standard software in the Python community, namely flake8 which adheres to the Python PEP8 code standards and additional rules.

\section{Project management}
    \subsection{Team}
    The author and developer of this thesis is Fabio Anderegg.
    
    \subsection{Stakeholders}
    The following other people are also involved in the project:
    
    \begin{tabular}{| p{4cm} | p{3.5cm} | p{8cm} |}
        \hline
        \textbf{Name} & \textbf{Institute} & \textbf{Description} \\ \hline
        Peter von Niederhäusern & Bern University of Applied Sciences & Tutor \\ \hline
        Mauricio Reyes & University of Bern & Customer of the thesis. 
        Head Healthcare Imaging A.I., Medical faculty of the University Bern. \\ \hline
        Andreas Spichiger & Bern University of Applied Sciences & External expert for the thesis. \\ \hline
        Alain Jungo & University of Bern & PhD student of Mauricio Reyes which works on the BraTS dataset for brain tumor segmentation. Technical expert and potential user of this thesis. \\ \hline
    \end{tabular}

\section{Infrastructure and technology}
This work will use the PyTorch \cite{paszke2017automatic} deep learning library. Most newer machine learning papers are written with PyTorch, because it is considered easier to learn and more powerful than TensorFlow and Keras \cite{pytorchvstensorflow}.

Other machine learning libraries will be used on demand, for example:

\begin{tabular}{|p{3cm}|p{12.5cm}|}
    \hline
    \textbf{Library} & \textbf{Description} \\ \hline
    Scikit & Diverse kits of machine learning libraries \\ \hline
    Matplotlib & Library to generate graphs \\ \hline
    PIL/Pillow & Image manipulation library \\ \hline
    NumPy & Matrix manipulation library \\ \hline
    pandas & Dataframe library \\ \hline
    torchvision & PyTorch extension for computer vision problems. Contains models for common architectures and pretrained parameters for these networks. Also contains tools for data augmentation. \\ \hline
\end{tabular}

The development of the system will take place inside Jupyter notebooks. Jupyter notebooks allow a very fast test and development cycle. The notebook server will be running on a powerful desktop computer of the author which is exposed to the internet, so the computational power is always available independent of the work location.

In addition, the GPU servers from the Bern University of Applied Sciences (NVIDIA DGX-1, 4x Tesla V100) and from the Medical Faculty of the University of Bern (Unknown number and type of GPUs) are available. Because the required time to setup the code and infrastructure to use these servers is quite high, the usage of these systems is optional and will only be done if the reduced time to train the neural network is worth the additional work effort.


\section{Appendix}
\subsection{Description of thesis as submitted}

Machine learning (ML) systems are achieving remarkable performances at the cost of increased complexity. Hence, they become less interpretable, which may cause distrust. As these systems are pervasively being introduced to critical domains, such as medical image computing and computer assisted intervention (MICCAI), it becomes imperative to develop methodologies to explain their predictions. Such methodologies would help physicians to decide whether they should follow/trust a prediction or not. Additionally, it could facilitate the deployment of such systems, from a legal perspective. Ultimately, interpretability is closely related with AI safety in healthcare.

In this thesis work the objective is to develop a software suite enabling advance interpretability of machine learning (ML) approaches. The software suite builds on recent developments to visualize and harness explicability of complex machine learning systems, namely, occlusion tests during training of an ML model, L.I.M.E Ribeiro et al. (\url{https://homes.cs.washington.edu/~marcotcr/blog/lime/}), and visualization tools from Zeiler et al. 2013 (\url{https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf} and Github at: \url{https://github.com/InFoCusp/tf\_cnnvis}).
 
As part of the thesis, a machine learning model for the BraTS dataset (benchmarking of Brain Tumor Segmentation) will be built. The algorithms described above (L.I.M.E, visualization tools) will then be used to inspect and evaluate how much these techniques help when building, optimizing and especially interpreting such a model in the medical imaging field.

\pagebreak
\subsection{Project planning Gantt chart}

The following Gantt chart shows the time planning for the thesis based on the goals and detail planning described above.
The green bar shows the 6 weeks where the author works full time on the thesis and where most of the programming work should be done.
The dark blue bars show the work packages as definied in the detailed project plan chapter.
The light blue charts represent optional goals, which also function as reserve time if another goal needs more time than anticipated.
The red circles represent important milestones of the project.

\begin{ganttchart}[
bar/.style={fill=bar},
vgrid,
hgrid,
milestone/.append style={anchor=east,xshift=-1pt,fill=milestone,shape=circle,draw=milestone,},
title height=1.0,
x unit=6mm,
y unit title=8mm,
y unit chart=8mm,
]{10}{25}
    \gantttitle{Calendar weeks 2019}{16} \\
    \gantttitlelist{10,...,25}{1} \\
    \ganttbar[bar/.append style={fill=fulltimebar}]{Full-time work on thesis}{10}{15} \ganttnewline

    \ganttbar{Train chest X-Ray model}{10}{11} \ganttnewline
    \ganttbar{Apply interpretability methods}{11}{11} \ganttnewline
    \ganttmilestone{Goals definied}{11} \ganttnewline
    \ganttbar{Neural network for brain tumor dataset}{12}{12} \ganttnewline
    
    \ganttbar{Modify RISE method}{13}{14} \ganttnewline
    \ganttbar{Modify Grad-CAM method}{14}{15} \ganttnewline
    
    \ganttbar[bar/.append style={fill=optionalbar}]{Additional black box methods}{16}{18} \ganttnewline
    \ganttbar[bar/.append style={fill=optionalbar}]{Additional white box methods}{16}{18} \ganttnewline
    \ganttbar{Build Python library}{19}{19} \ganttnewline
    
    \ganttbar{Finish thesis text}{20}{22} \ganttnewline

    \ganttmilestone{Draft thesis text done}{22} \ganttnewline
    \ganttbar{Presentation preparation}{23}{24} \ganttnewline
    \ganttmilestone{Deadline thesis text \& presentation}{24} \ganttnewline
    \ganttmilestone{Thesis defense}{25}
\end{ganttchart}