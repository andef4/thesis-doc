\chapter*{Abstract}
\label{chap:managementSummary}


Machine learning (ML) systems are achieving remarkable performances at the cost of increased complexity. Hence, they become less interpretable, which may cause distrust. As these systems are pervasively being introduced to critical domains, such as medical image computing and computer assisted intervention (MICCAI), it becomes imperative to develop methodologies to explain their predictions. Such methodologies would help physicians to decide whether they should follow/trust a prediction or not. Additionally, it could facilitate the deployment of such systems, from a legal perspective. Ultimately, interpretability is closely related with AI safety in healthcare.



%In this thesis work the objective is to develop a software suite enabling advance interpretability of machine learning (ML) approaches. The software suite builds on recent developments to visualize and harness explicability of complex machine learning systems, namely, occlusion tests during training of an ML model, L.I.M.E Ribeiro et al. (https://homes.cs.washington.edu/~marcotcr/blog/lime/), and visualization tools from Zeiler et al. 2013 (https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf and Github at: https://github.com/InFoCusp/tf\_cnnvis).
 
%As part of the thesis, a machine learning model for the BraTS dataset (benchmarking of Brain Tumor Segmentation) will be built. The algorithms described above (L.I.M.E, visualization tools) will then be used to inspect and evaluate how much these techniques help when building, optimizing and especially interpreting such a model in the medical imaging field.


%TODO: rework text
%In this thesis work the objective is to develop a software suite enabling advance interpretability of machine learning (ML) approaches. The software suite builds on recent developments to visualize and harness explicability of complex machine learning systems, namely, occlusion tests during training of an ML model, L.I.M.E Ribeiro et al., and visualization tools from Zeiler et al. 2013.
 
%As part of the thesis, a machine learning model for the BraTS dataset (benchmarking of Brain Tumor Segmentation) will be built. The algorithms described above (L.I.M.E, visualization tools) will then be used to inspect and evaluate how much these techniques help when building, optimizing and especially interpreting such a model in the medical imaging field.




why?
brats training
rise
hdm