\chapter*{Abstract}
\label{chap:managementSummary}

Machine learning (ML) systems are achieving remarkable performances at the cost of increased complexity. Hence, they become less interpretable, which may cause distrust. As these systems are pervasively being introduced to critical domains, such as medical image computing and computer assisted intervention (MICCAI), it becomes imperative to develop methodologies to explain their predictions. Such methodologies would help physicians to decide whether they should follow/trust a prediction or not. Additionally, it could facilitate the deployment of such systems, from a legal perspective. Ultimately, interpretability is closely related with AI safety in healthcare.

Interpretability methods for (image) classification tasks are already well established. LIME, RISE, Grad-CAM and other methods deliver deep insights how a (convolutional) neural network came to its conclusion. Interpretability methods for image segmentation tasks in comparison are almost non-existent. The goal of this thesis is to modify an existing algorithm for classification to also work for segmentation or come up with a new method.

As part of the thesis, a machine learning model for the BraTS dataset (benchmarking of Brain Tumor Segmentation) has been trained. This model has been used to asses the newly developed methods.

Two methods have been successfully implemented: The existing method RISE (Vitali Petsiuk et al.) has been modified to work on image segmentation tasks. In addition, an new method named Hausdorff Distance Mask has been developed. In essence, it works similar to to the occlusion method proposed by Zeiler et al. by occluding parts of the image and then comparing the network output to the baseline output from the unmodified original image. A visualization technique to generate a intuitively understandable graphic for interpretation is also provided.